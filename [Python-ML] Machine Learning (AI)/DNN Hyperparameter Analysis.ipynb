{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 134,794\n",
      "Trainable params: 134,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 1.6413 - accuracy: 0.5604 - val_loss: 0.8180 - val_accuracy: 0.8107\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 4s 84us/step - loss: 0.6144 - accuracy: 0.8380 - val_loss: 0.4514 - val_accuracy: 0.8742\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.4315 - accuracy: 0.8794 - val_loss: 0.3646 - val_accuracy: 0.8968\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.3670 - accuracy: 0.8964 - val_loss: 0.3243 - val_accuracy: 0.9075\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.3314 - accuracy: 0.9059 - val_loss: 0.2981 - val_accuracy: 0.9147\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 5s 108us/step - loss: 0.3062 - accuracy: 0.9121 - val_loss: 0.2774 - val_accuracy: 0.9209\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 5s 106us/step - loss: 0.2869 - accuracy: 0.9171 - val_loss: 0.2638 - val_accuracy: 0.9254\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 5s 104us/step - loss: 0.2701 - accuracy: 0.9223 - val_loss: 0.2497 - val_accuracy: 0.9286\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 5s 104us/step - loss: 0.2564 - accuracy: 0.9266 - val_loss: 0.2398 - val_accuracy: 0.9311\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.2434 - accuracy: 0.9294 - val_loss: 0.2301 - val_accuracy: 0.9339\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 7s 139us/step - loss: 0.2322 - accuracy: 0.9327 - val_loss: 0.2196 - val_accuracy: 0.9365\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.2214 - accuracy: 0.9356 - val_loss: 0.2142 - val_accuracy: 0.9395\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 6s 118us/step - loss: 0.2115 - accuracy: 0.9386 - val_loss: 0.2026 - val_accuracy: 0.9430\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 5s 105us/step - loss: 0.2024 - accuracy: 0.9413 - val_loss: 0.1979 - val_accuracy: 0.9444\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 5s 112us/step - loss: 0.1936 - accuracy: 0.9443 - val_loss: 0.1931 - val_accuracy: 0.9443\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 6s 120us/step - loss: 0.1859 - accuracy: 0.9463 - val_loss: 0.1847 - val_accuracy: 0.9483\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.1783 - accuracy: 0.9482 - val_loss: 0.1777 - val_accuracy: 0.9494\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.1715 - accuracy: 0.9505 - val_loss: 0.1720 - val_accuracy: 0.9522\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 0.1648 - accuracy: 0.9521 - val_loss: 0.1678 - val_accuracy: 0.9531\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.1588 - accuracy: 0.9541 - val_loss: 0.1624 - val_accuracy: 0.9548\n",
      "10000/10000 [==============================] - 0s 48us/step\n",
      "Test score: 0.16248419483676552\n",
      "Test accuracy: 0.95169997215271\n",
      "Training accuracy: 0.9541042\n",
      "Validation accuracy: 0.9548333287239075\n"
     ]
    }
   ],
   "source": [
    "from __future__         import print_function\n",
    "import numpy            as np\n",
    "#import tensorflow       as tf\n",
    "from keras.datasets     import mnist                                # image dataset of handwritten numbers\n",
    "from keras.models       import Sequential                           # model type\n",
    "from keras.layers       import Dense, Activation, Dropout, Input    #from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers   import SGD                                  # activation function\n",
    "from keras.utils        import to_categorical                       # one-hot encoding of ground truth values\n",
    "# NOTE: TensorFlow backend used by Keras\n",
    "\n",
    "np.random.seed(1671)  # for reproducibility\n",
    "\n",
    "# network & training hyperparameters\n",
    "VERBOSE = 1         # 0: no output, 1: progress bar, 2: one line per epoch\n",
    "N_CLASSES = 10      # number of outputs = number of digits\n",
    "# TODO: increase parameter values to improve accuracy\n",
    "N_EPOCHS = 20       # number of times the model is trained\n",
    "BATCH_SIZE = 128    # number of samples per gradient update before updating the weights\n",
    "N_HIDDEN = 128      # number of neurons in each hidden layer\n",
    "OPTIMIZER = SGD()   # optimizer  TODO: try Adam or RMSprop\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved for VALIDATION\n",
    "# TODO: DROPOUT = 0.3   # regularization to prevent overfitting\n",
    "\n",
    "# data: shuffled & split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()  # returns training & test data as tuples\n",
    "# X_train is 60000 rows (images) of 28x28 values (array) + labels ---> N_INPUT_DIMS in 60000 x 784\n",
    "# X_test is 10000 rows (images) of 28x28 values (array) + labels ---> N_INPUT_DIMS in 10000 x 784\n",
    "N_INPUT_DIMS = 784  # flattened 2D array to 1D array\n",
    "X_train = X_train.reshape(60000, N_INPUT_DIMS)  # 60000 x 784\n",
    "X_test = X_test.reshape(10000, N_INPUT_DIMS)    # 10000 x 784\n",
    "X_train = X_train.astype('float32')         # change type\n",
    "X_test = X_test.astype('float32')           # change type\n",
    "# normalize\n",
    "X_train /= 255  # 0-255 to 0-1\n",
    "X_test /= 255   # 0-255 to 0-1\n",
    "\n",
    "print(X_train.shape[0], 'train samples')    # number of rows\n",
    "print(X_test.shape[0], 'test samples')      # number of rows\n",
    "\n",
    "#convert class vectors to binary class matrices\n",
    "Y_train = to_categorical(y_train, N_CLASSES)  # 10 outputs\n",
    "Y_test = to_categorical(y_test, N_CLASSES)    # 10 outputs\n",
    "\n",
    "model = Sequential()    # model with linear stack of layers\n",
    "\n",
    "# input layer\n",
    "model.add(Dense(N_HIDDEN, input_shape=(N_INPUT_DIMS,))) # 784 input features connected to each of the 128 neurons\n",
    "model.add(Activation('relu'))       # passes positive numbers unchanged and negative ones as 0\n",
    "# TODO: model.add(Dropout(DROPOUT))\n",
    "\n",
    "# hidden layer #1\n",
    "model.add(Dense(N_HIDDEN))          # hidden layer with 128 neurons\n",
    "model.add(Activation('relu'))       # passes positive numbers unchanged and negative ones as 0\n",
    "# TODO: model.add(Dropout(DROPOUT))\n",
    "\n",
    "# hidden layer #2\n",
    "model.add(Dense(N_HIDDEN))          # hidden layer with 128 neurons\n",
    "model.add(Activation('relu'))       # passes positive numbers unchanged and negative ones as 0\n",
    "# TODO: model.add(Dropout(DROPOUT))\n",
    "\n",
    "# TODO: add more hidden dense layers and dropouts\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(N_CLASSES))         # output layer with 10 outputs (each possible digit)\n",
    "model.add(Activation('softmax'))    # converts outputs into probabilities (confidences) for each class that sum to 1\n",
    "\n",
    "''' optional model & layer creation\n",
    "model = Sequential([\n",
    "    Input(shape=(N_INPUT_DIMS,)),               # input layer with number of neurons\n",
    "    Dense(N_HIDDEN, activation='relu'),     # input layer with number of neurons & relu activation\n",
    "    # TODO: add more hidden dense layers and dropouts here\n",
    "    Dense(N_CLASSES, activation='softmax')  # output layer with number of outputs (each possible digit) & softmax activation\n",
    "])  \n",
    "'''\n",
    "\n",
    "# Evaluate model\n",
    "model.summary()     # prints a summary representation of the model\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',  # loss function (classification predictions)\n",
    "              optimizer=OPTIMIZER,              # optimizer\n",
    "              metrics=['accuracy'])             # metrics to be evaluated by the model during training and testing\n",
    "\n",
    "history = model.fit(X_train, Y_train,                                   # training data\n",
    "                    batch_size=BATCH_SIZE, epochs=N_EPOCHS,             # number of times the model is trained\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT) # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)     # test the model\n",
    "\n",
    "print(\"Training accuracy:\", history.history['accuracy'][-1])        # final training accuracy\n",
    "print(\"Test accuracy:\", score[1])  # test accuracy\n",
    "print(\"Test score:\", score[0])     # loss on test\n",
    "print(\"Validation accuracy:\", history.history['val_accuracy'][-1])  # final validation accuracy\n",
    "\n",
    "# Goal: high training and validation accuracy (which means good generalization)\n",
    "# Overfitting: large gap between training and validation accuracy (try L1 regression to nullify unaffective nodes or L2 for colinearity\n",
    "# Underfitting: low training and validation accuracy\n",
    "#------------------------------------------------\n",
    "# RESULTS (6 significant digits)\n",
    "#------------------------------------------------\n",
    "'''\n",
    "    BASELINE:           2 hidden layers, 128 neurons, 20 epochs, 128 batch size, 0.2 validation split,\n",
    "                        SGD optimizer, relu activation, categorical_crossentropy loss, 784 input shape, 10 outputs\n",
    "    Training accuracy:  0.954104\n",
    "    Test accuracy:      0.951700\n",
    "    Test score (loss):  0.162484  # lower loss means higher accuracy\n",
    "    Validation accuracy:0.954833  # indicator of generalization accuracy\n",
    "'''\n",
    "pass    # omits outputting last unassigned string (in ''' block above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_37 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 134,794\n",
      "Trainable params: 134,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 2.0517 - accuracy: 0.3833 - val_loss: 1.6288 - val_accuracy: 0.6415\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 1.1615 - accuracy: 0.7397 - val_loss: 0.7653 - val_accuracy: 0.8252\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.6510 - accuracy: 0.8320 - val_loss: 0.5146 - val_accuracy: 0.8689\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.4967 - accuracy: 0.8645 - val_loss: 0.4239 - val_accuracy: 0.8857\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.4278 - accuracy: 0.8814 - val_loss: 0.3772 - val_accuracy: 0.8939\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.3883 - accuracy: 0.8913 - val_loss: 0.3477 - val_accuracy: 0.9032\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.3620 - accuracy: 0.8971 - val_loss: 0.3292 - val_accuracy: 0.9056\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.3419 - accuracy: 0.9019 - val_loss: 0.3141 - val_accuracy: 0.9112\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.3263 - accuracy: 0.9065 - val_loss: 0.3037 - val_accuracy: 0.9129\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.3130 - accuracy: 0.9102 - val_loss: 0.2904 - val_accuracy: 0.9159\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.3017 - accuracy: 0.9121 - val_loss: 0.2824 - val_accuracy: 0.9188\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.2916 - accuracy: 0.9159 - val_loss: 0.2745 - val_accuracy: 0.9211\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.2825 - accuracy: 0.9180 - val_loss: 0.2651 - val_accuracy: 0.9248\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 4s 86us/step - loss: 0.2740 - accuracy: 0.9210 - val_loss: 0.2596 - val_accuracy: 0.9265\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 0.2664 - accuracy: 0.9230 - val_loss: 0.2544 - val_accuracy: 0.9271\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.2591 - accuracy: 0.9250 - val_loss: 0.2482 - val_accuracy: 0.9287\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 0.2523 - accuracy: 0.9264 - val_loss: 0.2400 - val_accuracy: 0.9303\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.2462 - accuracy: 0.9294 - val_loss: 0.2351 - val_accuracy: 0.9320\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.2396 - accuracy: 0.9306 - val_loss: 0.2314 - val_accuracy: 0.9334\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 4s 86us/step - loss: 0.2341 - accuracy: 0.9322 - val_loss: 0.2257 - val_accuracy: 0.9357\n",
      "10000/10000 [==============================] - 0s 50us/step\n",
      "Training accuracy: 0.932208\n",
      "Test accuracy: 0.9328\n",
      "Test score (loss): 0.229173\n",
      "Validation accuracy: 0.935667\n"
     ]
    }
   ],
   "source": [
    "from __future__         import print_function\n",
    "import numpy            as np\n",
    "#import tensorflow       as tf\n",
    "from keras.datasets     import mnist                                # image dataset of handwritten numbers\n",
    "from keras.models       import Sequential                           # model type\n",
    "from keras.layers       import Dense, Activation, Dropout, Input    #from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers   import SGD                                  # activation function\n",
    "from keras.utils        import to_categorical                       # one-hot encoding of ground truth values\n",
    "# NOTE: TensorFlow backend used by Keras\n",
    "\n",
    "np.random.seed(1671)  # for reproducibility\n",
    "\n",
    "# network & training hyperparameters\n",
    "VERBOSE = 1         # 0: no output, 1: progress bar, 2: one line per epoch\n",
    "N_CLASSES = 10      # number of outputs = number of digits\n",
    "# TODO: increase parameter values to improve accuracy\n",
    "N_EPOCHS = 20       # number of times the model is trained\n",
    "BATCH_SIZE = 256    # number of samples per gradient update before updating the weights\n",
    "N_HIDDEN = 128      # number of neurons in each hidden layer\n",
    "OPTIMIZER = SGD()   # optimizer  TODO: try Adam or RMSprop\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved for VALIDATION\n",
    "# TODO: DROPOUT = 0.3   # regularization to prevent overfitting\n",
    "\n",
    "# data: shuffled & split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()  # returns training & test data as tuples\n",
    "# X_train is 60000 rows (images) of 28x28 values (array) + labels ---> N_INPUT_DIMS in 60000 x 784\n",
    "# X_test is 10000 rows (images) of 28x28 values (array) + labels ---> N_INPUT_DIMS in 10000 x 784\n",
    "N_INPUT_DIMS = 784  # flattened 2D array to 1D array\n",
    "X_train = X_train.reshape(60000, N_INPUT_DIMS)  # 60000 x 784\n",
    "X_test = X_test.reshape(10000, N_INPUT_DIMS)    # 10000 x 784\n",
    "X_train = X_train.astype('float32')         # change type\n",
    "X_test = X_test.astype('float32')           # change type\n",
    "# normalize\n",
    "X_train /= 255  # 0-255 to 0-1\n",
    "X_test /= 255   # 0-255 to 0-1\n",
    "\n",
    "print(X_train.shape[0], 'train samples')    # number of rows\n",
    "print(X_test.shape[0], 'test samples')      # number of rows\n",
    "\n",
    "#convert class vectors to binary class matrices\n",
    "Y_train = to_categorical(y_train, N_CLASSES)  # 10 outputs\n",
    "Y_test = to_categorical(y_test, N_CLASSES)    # 10 outputs\n",
    "\n",
    "model = Sequential()    # model with linear stack of layers\n",
    "\n",
    "# input layer\n",
    "model.add(Dense(N_HIDDEN, input_shape=(N_INPUT_DIMS,))) # 784 input features connected to each of the 128 neurons\n",
    "model.add(Activation('relu'))       # passes positive numbers unchanged and negative ones as 0\n",
    "# TODO: model.add(Dropout(DROPOUT))\n",
    "\n",
    "# hidden layer #1\n",
    "model.add(Dense(N_HIDDEN))          # hidden layer with 128 neurons\n",
    "model.add(Activation('relu'))       # passes positive numbers unchanged and negative ones as 0\n",
    "# TODO: model.add(Dropout(DROPOUT))\n",
    "\n",
    "# hidden layer #2\n",
    "model.add(Dense(N_HIDDEN))          # hidden layer with 128 neurons\n",
    "model.add(Activation('relu'))       # passes positive numbers unchanged and negative ones as 0\n",
    "# TODO: model.add(Dropout(DROPOUT))\n",
    "\n",
    "# TODO: add more hidden dense layers and dropouts\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(N_CLASSES))         # output layer with 10 outputs (each possible digit)\n",
    "model.add(Activation('softmax'))    # converts outputs into probabilities (confidences) for each class that sum to 1\n",
    "\n",
    "''' optional model & layer creation\n",
    "model = Sequential([\n",
    "    Input(shape=(N_INPUT_DIMS,)),           # input layer with number of neurons\n",
    "    Dense(N_HIDDEN, activation='relu'),     # input layer with number of neurons & relu activation\n",
    "    # TODO: add more hidden dense layers and dropouts here\n",
    "    Dense(N_CLASSES, activation='softmax')  # output layer with number of outputs (each possible digit) & softmax activation\n",
    "])  \n",
    "'''\n",
    "\n",
    "# Evaluate model\n",
    "model.summary()     # prints a summary representation of the model\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',  # loss function\n",
    "              optimizer=OPTIMIZER,              # optimizer\n",
    "              metrics=['accuracy'])             # metrics to be evaluated by the model during training and testing\n",
    "\n",
    "history = model.fit(X_train, Y_train,                                   # training data\n",
    "                    batch_size=BATCH_SIZE, epochs=N_EPOCHS,             # number of times the model is trained\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT) # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)     # test the model\n",
    "\n",
    "print(\"Training accuracy:\", round(history.history['accuracy'][-1],6))        # final training accuracy\n",
    "print(\"Test accuracy:\", round(score[1],6))         # test accuracy\n",
    "print(\"Test score (loss):\", round(score[0],6))     # loss on test\n",
    "print(\"Validation accuracy:\", round(history.history['val_accuracy'][-1],6))  # final validation accuracy\n",
    "\n",
    "# Goal: high training and validation accuracy (which means good generalization)\n",
    "# Overfitting: large gap between training and validation accuracy (try L1 regression to nullify unaffective nodes or L2 for colinearity\n",
    "# Underfitting: low training and validation accuracy\n",
    "#------------------------------------------------\n",
    "# RESULTS (6 significant digits)\n",
    "#------------------------------------------------\n",
    "'''\n",
    "    BASELINE:           2 hidden layers, 128 neurons, 20 epochs, 128 batch size, 0.2 validation split,\n",
    "                        SGD optimizer, relu activation, categorical_crossentropy loss, 784 input shape, 10 outputs\n",
    "    \n",
    "    Training accuracy:  0.954104\n",
    "    Test accuracy:      0.951700\n",
    "    Test score (loss):  0.162484  # lower loss means higher accuracy\n",
    "    Validation accuracy:0.954833  # indicator of generalization accuracy\n",
    "\n",
    "    Experiments:        hyperparameter=BATCH_SIZE : original=128\n",
    "    -------------------------------------------------------------\n",
    "    EXP #1:             new=256\n",
    "    Training accuracy:  0.932208\n",
    "    Test accuracy:      0.9328\n",
    "    Test score (loss):  0.229173\n",
    "    Validation accuracy:0.935667\n",
    "'''\n",
    "pass    # omits outputting last unassigned string (in ''' block above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 134,794\n",
      "Trainable params: 134,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 31s 646us/step - loss: 0.3981 - accuracy: 0.8845 - val_loss: 0.1886 - val_accuracy: 0.9454\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 35s 723us/step - loss: 0.1673 - accuracy: 0.9498 - val_loss: 0.1522 - val_accuracy: 0.9567\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 30s 624us/step - loss: 0.1171 - accuracy: 0.9641 - val_loss: 0.1136 - val_accuracy: 0.9663\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 35s 719us/step - loss: 0.0911 - accuracy: 0.9715 - val_loss: 0.1076 - val_accuracy: 0.9666\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 36s 760us/step - loss: 0.0726 - accuracy: 0.9772 - val_loss: 0.0939 - val_accuracy: 0.9732\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 40s 836us/step - loss: 0.0607 - accuracy: 0.9810 - val_loss: 0.0892 - val_accuracy: 0.9742\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 44s 920us/step - loss: 0.0484 - accuracy: 0.9848 - val_loss: 0.0916 - val_accuracy: 0.9723\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 40s 830us/step - loss: 0.0404 - accuracy: 0.9877 - val_loss: 0.0941 - val_accuracy: 0.9744\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 41s 845us/step - loss: 0.0338 - accuracy: 0.9897 - val_loss: 0.0975 - val_accuracy: 0.9730\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 36s 751us/step - loss: 0.0287 - accuracy: 0.9911 - val_loss: 0.0886 - val_accuracy: 0.9761\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 35s 726us/step - loss: 0.0233 - accuracy: 0.9933 - val_loss: 0.0980 - val_accuracy: 0.9731\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 34s 706us/step - loss: 0.0194 - accuracy: 0.9946 - val_loss: 0.0938 - val_accuracy: 0.9756\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 36s 746us/step - loss: 0.0156 - accuracy: 0.9956 - val_loss: 0.0967 - val_accuracy: 0.9775\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 44s 919us/step - loss: 0.0119 - accuracy: 0.9970 - val_loss: 0.0992 - val_accuracy: 0.9763\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 48s 991us/step - loss: 0.0108 - accuracy: 0.9971 - val_loss: 0.1012 - val_accuracy: 0.9759\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 43s 893us/step - loss: 0.0079 - accuracy: 0.9982 - val_loss: 0.0979 - val_accuracy: 0.9769\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 38s 783us/step - loss: 0.0051 - accuracy: 0.9992 - val_loss: 0.1032 - val_accuracy: 0.9765\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 40s 831us/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.0998 - val_accuracy: 0.9773\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 39s 804us/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.0989 - val_accuracy: 0.9773\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 39s 803us/step - loss: 0.0020 - accuracy: 0.9999 - val_loss: 0.0992 - val_accuracy: 0.9778\n",
      "10000/10000 [==============================] - 1s 53us/step\n",
      "Training accuracy: 0.999875\n",
      "Test accuracy: 0.9798\n",
      "Test score (loss): 0.086414\n",
      "Validation accuracy: 0.97775\n"
     ]
    }
   ],
   "source": [
    "from __future__         import print_function\n",
    "import numpy            as np\n",
    "#import tensorflow       as tf\n",
    "from keras.datasets     import mnist                                # image dataset of handwritten numbers\n",
    "from keras.models       import Sequential                           # model type\n",
    "from keras.layers       import Dense, Activation, Dropout, Input    #from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers   import SGD                                  # activation function\n",
    "from keras.utils        import to_categorical                       # one-hot encoding of ground truth values\n",
    "# NOTE: TensorFlow backend used by Keras\n",
    "\n",
    "np.random.seed(1671)  # for reproducibility\n",
    "\n",
    "# network & training hyperparameters\n",
    "VERBOSE = 1         # 0: no output, 1: progress bar, 2: one line per epoch\n",
    "N_CLASSES = 10      # number of outputs = number of digits\n",
    "# TODO: increase parameter values to improve accuracy\n",
    "N_EPOCHS = 20       # number of times the model is trained\n",
    "BATCH_SIZE = 8    # number of samples per gradient update before updating the weights\n",
    "N_HIDDEN = 128      # number of neurons in each hidden layer\n",
    "OPTIMIZER = SGD()   # optimizer  TODO: try Adam or RMSprop\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved for VALIDATION\n",
    "# TODO: DROPOUT = 0.3   # regularization to prevent overfitting\n",
    "\n",
    "# data: shuffled & split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()  # returns training & test data as tuples\n",
    "# X_train is 60000 rows (images) of 28x28 values (array) + labels ---> N_INPUT_DIMS in 60000 x 784\n",
    "# X_test is 10000 rows (images) of 28x28 values (array) + labels ---> N_INPUT_DIMS in 10000 x 784\n",
    "N_INPUT_DIMS = 784  # flattened 2D array to 1D array\n",
    "X_train = X_train.reshape(60000, N_INPUT_DIMS)  # 60000 x 784\n",
    "X_test = X_test.reshape(10000, N_INPUT_DIMS)    # 10000 x 784\n",
    "X_train = X_train.astype('float32')         # change type\n",
    "X_test = X_test.astype('float32')           # change type\n",
    "# normalize\n",
    "X_train /= 255  # 0-255 to 0-1\n",
    "X_test /= 255   # 0-255 to 0-1\n",
    "\n",
    "print(X_train.shape[0], 'train samples')    # number of rows\n",
    "print(X_test.shape[0], 'test samples')      # number of rows\n",
    "\n",
    "#convert class vectors to binary class matrices\n",
    "Y_train = to_categorical(y_train, N_CLASSES)  # 10 outputs\n",
    "Y_test = to_categorical(y_test, N_CLASSES)    # 10 outputs\n",
    "\n",
    "model = Sequential()    # model with linear stack of layers\n",
    "\n",
    "# input layer\n",
    "model.add(Dense(N_HIDDEN, input_shape=(N_INPUT_DIMS,))) # 784 input features connected to each of the 128 neurons\n",
    "model.add(Activation('relu'))       # passes positive numbers unchanged and negative ones as 0\n",
    "# TODO: model.add(Dropout(DROPOUT))\n",
    "\n",
    "# hidden layer #1\n",
    "model.add(Dense(N_HIDDEN))          # hidden layer with 128 neurons\n",
    "model.add(Activation('relu'))       # passes positive numbers unchanged and negative ones as 0\n",
    "# TODO: model.add(Dropout(DROPOUT))\n",
    "\n",
    "# hidden layer #2\n",
    "model.add(Dense(N_HIDDEN))          # hidden layer with 128 neurons\n",
    "model.add(Activation('relu'))       # passes positive numbers unchanged and negative ones as 0\n",
    "# TODO: model.add(Dropout(DROPOUT))\n",
    "\n",
    "# TODO: add more hidden dense layers and dropouts\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(N_CLASSES))         # output layer with 10 outputs (each possible digit)\n",
    "model.add(Activation('softmax'))    # converts outputs into probabilities (confidences) for each class that sum to 1\n",
    "\n",
    "''' optional model & layer creation\n",
    "model = Sequential([\n",
    "    Input(shape=(N_INPUT_DIMS,)),           # input layer with number of neurons\n",
    "    Dense(N_HIDDEN, activation='relu'),     # input layer with number of neurons & relu activation\n",
    "    # TODO: add more hidden dense layers and dropouts here\n",
    "    Dense(N_CLASSES, activation='softmax')  # output layer with number of outputs (each possible digit) & softmax activation\n",
    "])  \n",
    "'''\n",
    "\n",
    "# Evaluate model\n",
    "model.summary()     # prints a summary representation of the model\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',  # loss function\n",
    "              optimizer=OPTIMIZER,              # optimizer\n",
    "              metrics=['accuracy'])             # metrics to be evaluated by the model during training and testing\n",
    "\n",
    "history = model.fit(X_train, Y_train,                                   # training data\n",
    "                    batch_size=BATCH_SIZE, epochs=N_EPOCHS,             # number of times the model is trained\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT) # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)     # test the model\n",
    "\n",
    "print(\"Training accuracy:\", round(history.history['accuracy'][-1],6))        # final training accuracy\n",
    "print(\"Test accuracy:\", round(score[1],6))         # test accuracy\n",
    "print(\"Test score (loss):\", round(score[0],6))     # loss on test\n",
    "print(\"Validation accuracy:\", round(history.history['val_accuracy'][-1],6))  # final validation accuracy\n",
    "\n",
    "# Goal: high training and validation accuracy (which means good generalization)\n",
    "# Overfitting: large gap between training and validation accuracy (try L1 regression to nullify unaffective nodes or L2 for colinearity\n",
    "# Underfitting: low training and validation accuracy\n",
    "#------------------------------------------------\n",
    "# RESULTS (6 significant digits)\n",
    "#------------------------------------------------\n",
    "'''\n",
    "    BASELINE:           2 hidden layers, 128 neurons, 20 epochs, 128 batch size, 0.2 validation split,\n",
    "                        SGD optimizer, relu activation, categorical_crossentropy loss, 784 input shape, 10 outputs\n",
    "    \n",
    "    Training accuracy:  0.954104\n",
    "    Test accuracy:      0.951700\n",
    "    Test score (loss):  0.162484  # lower loss means higher accuracy\n",
    "    Validation accuracy:0.954833  # indicator of generalization accuracy\n",
    "\n",
    "    Experiments:        hyperparameter=BATCH_SIZE : original=128\n",
    "    -------------------------------------------------------------\n",
    "    EXP #1:             new=256\n",
    "    Training accuracy:  0.932208\n",
    "    Test accuracy:      0.9328\n",
    "    Test score (loss):  0.229173\n",
    "    Validation accuracy:0.935667\n",
    "    \n",
    "    EXP #2:             new=8\n",
    "    Training accuracy:  0.999875\n",
    "    Test accuracy:      0.9798\n",
    "    Test score (loss):  0.086414\n",
    "    Validation accuracy:0.97775\n",
    "'''\n",
    "pass    # omits outputting last unassigned string (in ''' block above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 134,794\n",
      "Trainable params: 134,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 194s 4ms/step - loss: 0.2608 - accuracy: 0.9181 - val_loss: 0.1652 - val_accuracy: 0.9523\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 212s 4ms/step - loss: 0.1249 - accuracy: 0.9627 - val_loss: 0.1440 - val_accuracy: 0.9593\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 246s 5ms/step - loss: 0.0883 - accuracy: 0.9727 - val_loss: 0.1076 - val_accuracy: 0.9699\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 183s 4ms/step - loss: 0.0712 - accuracy: 0.9782 - val_loss: 0.0998 - val_accuracy: 0.9701\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 193s 4ms/step - loss: 0.0574 - accuracy: 0.9816 - val_loss: 0.1277 - val_accuracy: 0.9672\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 192s 4ms/step - loss: 0.0483 - accuracy: 0.9847 - val_loss: 0.1100 - val_accuracy: 0.9723\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 176s 4ms/step - loss: 0.0426 - accuracy: 0.9862 - val_loss: 0.1212 - val_accuracy: 0.9707\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 176s 4ms/step - loss: 0.0358 - accuracy: 0.9884 - val_loss: 0.1101 - val_accuracy: 0.9737\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 139s 3ms/step - loss: 0.0325 - accuracy: 0.9898 - val_loss: 0.1179 - val_accuracy: 0.9717\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 144s 3ms/step - loss: 0.0258 - accuracy: 0.9919 - val_loss: 0.1155 - val_accuracy: 0.9761\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 136s 3ms/step - loss: 0.0245 - accuracy: 0.9926 - val_loss: 0.1040 - val_accuracy: 0.9779\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 181s 4ms/step - loss: 0.0238 - accuracy: 0.9925 - val_loss: 0.1124 - val_accuracy: 0.9739\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 171s 4ms/step - loss: 0.0279 - accuracy: 0.9913 - val_loss: 0.1096 - val_accuracy: 0.9768\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 171s 4ms/step - loss: 0.0192 - accuracy: 0.9940 - val_loss: 0.1259 - val_accuracy: 0.9753\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 160s 3ms/step - loss: 0.0227 - accuracy: 0.9933 - val_loss: 0.1162 - val_accuracy: 0.9757\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 179s 4ms/step - loss: 0.0172 - accuracy: 0.9946 - val_loss: 0.1165 - val_accuracy: 0.9757\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 185s 4ms/step - loss: 0.0172 - accuracy: 0.9947 - val_loss: 0.1184 - val_accuracy: 0.9781\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 198s 4ms/step - loss: 0.0176 - accuracy: 0.9942 - val_loss: 0.1160 - val_accuracy: 0.9783\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 236s 5ms/step - loss: 0.0164 - accuracy: 0.9952 - val_loss: 0.1369 - val_accuracy: 0.9750\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 231s 5ms/step - loss: 0.0120 - accuracy: 0.9964 - val_loss: 0.1216 - val_accuracy: 0.9772\n",
      "10000/10000 [==============================] - 0s 36us/step\n",
      "Training accuracy: 0.996354\n",
      "Test accuracy: 0.9749\n",
      "Test score (loss): 0.127938\n",
      "Validation accuracy: 0.97725\n"
     ]
    }
   ],
   "source": [
    "from __future__         import print_function\n",
    "import numpy            as np\n",
    "#import tensorflow       as tf\n",
    "from keras.datasets     import mnist                                # image dataset of handwritten numbers\n",
    "from keras.models       import Sequential                           # model type\n",
    "from keras.layers       import Dense, Activation, Dropout, Input    #from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers   import SGD                                  # activation function\n",
    "from keras.utils        import to_categorical                       # one-hot encoding of ground truth values\n",
    "# NOTE: TensorFlow backend used by Keras\n",
    "\n",
    "np.random.seed(1671)  # for reproducibility\n",
    "\n",
    "# network & training hyperparameters\n",
    "VERBOSE = 1         # 0: no output, 1: progress bar, 2: one line per epoch\n",
    "N_CLASSES = 10      # number of outputs = number of digits\n",
    "# TODO: increase parameter values to improve accuracy\n",
    "N_EPOCHS = 20       # number of times the model is trained\n",
    "BATCH_SIZE = 1      # number of samples per gradient update before updating the weights\n",
    "N_HIDDEN = 128      # number of neurons in each hidden layer\n",
    "OPTIMIZER = SGD()   # optimizer  TODO: try Adam or RMSprop\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved for VALIDATION\n",
    "# TODO: DROPOUT = 0.3   # regularization to prevent overfitting\n",
    "\n",
    "# data: shuffled & split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()  # returns training & test data as tuples\n",
    "# X_train is 60000 rows (images) of 28x28 values (array) + labels ---> N_INPUT_DIMS in 60000 x 784\n",
    "# X_test is 10000 rows (images) of 28x28 values (array) + labels ---> N_INPUT_DIMS in 10000 x 784\n",
    "N_INPUT_DIMS = 784  # flattened 2D array to 1D array\n",
    "X_train = X_train.reshape(60000, N_INPUT_DIMS)  # 60000 x 784\n",
    "X_test = X_test.reshape(10000, N_INPUT_DIMS)    # 10000 x 784\n",
    "X_train = X_train.astype('float32')         # change type\n",
    "X_test = X_test.astype('float32')           # change type\n",
    "# normalize\n",
    "X_train /= 255  # 0-255 to 0-1\n",
    "X_test /= 255   # 0-255 to 0-1\n",
    "\n",
    "print(X_train.shape[0], 'train samples')    # number of rows\n",
    "print(X_test.shape[0], 'test samples')      # number of rows\n",
    "\n",
    "#convert class vectors to binary class matrices\n",
    "Y_train = to_categorical(y_train, N_CLASSES)  # 10 outputs\n",
    "Y_test = to_categorical(y_test, N_CLASSES)    # 10 outputs\n",
    "\n",
    "model = Sequential()    # model with linear stack of layers\n",
    "\n",
    "# input layer\n",
    "model.add(Dense(N_HIDDEN, input_shape=(N_INPUT_DIMS,))) # 784 input features connected to each of the 128 neurons\n",
    "model.add(Activation('relu'))       # passes positive numbers unchanged and negative ones as 0\n",
    "# TODO: model.add(Dropout(DROPOUT))\n",
    "\n",
    "# hidden layer #1\n",
    "model.add(Dense(N_HIDDEN))          # hidden layer with 128 neurons\n",
    "model.add(Activation('relu'))       # passes positive numbers unchanged and negative ones as 0\n",
    "# TODO: model.add(Dropout(DROPOUT))\n",
    "\n",
    "# hidden layer #2\n",
    "model.add(Dense(N_HIDDEN))          # hidden layer with 128 neurons\n",
    "model.add(Activation('relu'))       # passes positive numbers unchanged and negative ones as 0\n",
    "# TODO: model.add(Dropout(DROPOUT))\n",
    "\n",
    "# TODO: add more hidden dense layers and dropouts\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(N_CLASSES))         # output layer with 10 outputs (each possible digit)\n",
    "model.add(Activation('softmax'))    # converts outputs into probabilities (confidences) for each class that sum to 1\n",
    "\n",
    "''' optional model & layer creation\n",
    "model = Sequential([\n",
    "    Input(shape=(N_INPUT_DIMS,)),           # input layer with number of neurons\n",
    "    Dense(N_HIDDEN, activation='relu'),     # input layer with number of neurons & relu activation\n",
    "    # TODO: add more hidden dense layers and dropouts here\n",
    "    Dense(N_CLASSES, activation='softmax')  # output layer with number of outputs (each possible digit) & softmax activation\n",
    "])  \n",
    "'''\n",
    "\n",
    "# Evaluate model\n",
    "model.summary()     # prints a summary representation of the model\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',  # loss function\n",
    "              optimizer=OPTIMIZER,              # optimizer\n",
    "              metrics=['accuracy'])             # metrics to be evaluated by the model during training and testing\n",
    "\n",
    "history = model.fit(X_train, Y_train,                                   # training data\n",
    "                    batch_size=BATCH_SIZE, epochs=N_EPOCHS,             # number of times the model is trained\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT) # how much TRAIN is reserved for VALIDATION\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)     # test the model\n",
    "\n",
    "print(\"Training accuracy:\", round(history.history['accuracy'][-1],6))        # final training accuracy\n",
    "print(\"Test accuracy:\", round(score[1],6))         # test accuracy\n",
    "print(\"Test score (loss):\", round(score[0],6))     # loss on test\n",
    "print(\"Validation accuracy:\", round(history.history['val_accuracy'][-1],6))  # final validation accuracy\n",
    "\n",
    "# Goal: high training and validation accuracy (which means good generalization)\n",
    "# Overfitting: large gap between training and validation accuracy (try L1 regression to nullify unaffective nodes or L2 for colinearity\n",
    "# Underfitting: low training and validation accuracy\n",
    "#------------------------------------------------\n",
    "# RESULTS (6 significant digits)\n",
    "#------------------------------------------------\n",
    "'''\n",
    "    BASELINE:           2 hidden layers, 128 neurons, 20 epochs, 128 batch size, 0.2 validation split,\n",
    "                        SGD optimizer, relu activation, categorical_crossentropy loss, 784 input shape, 10 outputs\n",
    "   \n",
    "    Training accuracy:  0.954104\n",
    "    Test accuracy:      0.951700\n",
    "    Test score (loss):  0.162484  # lower loss means higher accuracy\n",
    "    Validation accuracy:0.954833  # indicator of generalization accuracy\n",
    "\n",
    "    Experiments:        hyperparameter=BATCH_SIZE : original=128\n",
    "    -------------------------------------------------------------\n",
    "    EXP #1:             new=256 (WORST)\n",
    "    Training accuracy:  0.932208\n",
    "    Test accuracy:      0.9328\n",
    "    Test score (loss):  0.229173\n",
    "    Validation accuracy:0.935667\n",
    "    \n",
    "    EXP #2:             new=8 (BEST)\n",
    "    Training accuracy:  0.999875\n",
    "    Test accuracy:      0.9798\n",
    "    Test score (loss):  0.086414\n",
    "    Validation accuracy:0.97775\n",
    "    \n",
    "    EXP #3:             new=1 (SLOWEST)\n",
    "    Training accuracy:  0.996354\n",
    "    Test accuracy:      0.9749\n",
    "    Test score (loss):  0.127938\n",
    "    Validation accuracy:0.97725\n",
    "'''\n",
    "pass    # omits outputting last unassigned string (in ''' block above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#--------------------------------------------------#\n",
    "# RESULTS (6 significant digits)\n",
    "#--------------------------------------------------#\n",
    "    BASELINE:\n",
    "        2 hidden layers, 128 neurons, 20 epochs, 128 batch size, 0.2 validation split,\n",
    "        SGD optimizer, relu activation, categorical_crossentropy loss, 784 input shape, 10 outputs\n",
    "   \n",
    "        Training accuracy:  0.954104\n",
    "        Test accuracy:      0.951700\n",
    "        Test score (loss):  0.162484  # lower loss means higher accuracy\n",
    "        Validation accuracy:0.954833  # indicator of generalization accuracy\n",
    "\n",
    "    Experiments:        hyperparameter=BATCH_SIZE : original=128\n",
    "    -------------------------------------------------------------\n",
    "    EXP #1: new=256         (WORST)\n",
    "        Training accuracy:  0.932208\n",
    "        Test accuracy:      0.9328\n",
    "        Test score (loss):  0.229173\n",
    "        Validation accuracy:0.935667\n",
    "    \n",
    "    EXP #2: new=8           (BEST)\n",
    "        Training accuracy:  0.999875\n",
    "        Test accuracy:      0.9798\n",
    "        Test score (loss):  0.086414\n",
    "        Validation accuracy:0.97775\n",
    "    \n",
    "    EXP #3: new=1           (SLOWEST)\n",
    "        Training accuracy:  0.996354\n",
    "        Test accuracy:      0.9749\n",
    "        Test score (loss):  0.127938\n",
    "        Validation accuracy:0.97725\n",
    "------------------------------------------------------------------\n",
    "# CONCLUSION\n",
    "------------------------------------------------------------------\n",
    "After determining the baseline training & test results, I decided to experiment with the BATCH_SIZE hyperparameter, which is the number of samples per gradient update before updating the weights. When raising this parameter to 256 (twice the original value), the accuracy went down and the loss went up. When lowering this value to 1, the training took a substantially longer amount of time to complete, but offered impressive results. The best results came from a BATCH_SIZE of 8, which although still quite slow, offered the highest training, test, and validation accuracy, as well as the lowest loss score on the tests of 0.086515 - down from the original value of 0.162484! The model is inclined to be better at generalization than the original training permitted, as can be seen in the validation accuracy score of 0.97725 (opposed to 0.954833). Overall, it makes sense that lowering the BATCH_SIZE value will increase the frequency of weight updates, and thus take a longer amount of time to complete. But since the weights are being adjusted so frequently, the model is able to better adjust to the training, resulting in a higher overall accuracy rating.In conclusion, changing the hyperparameter BATCH_SIZE resulted in a model with greater accuracy and generalization skills."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
